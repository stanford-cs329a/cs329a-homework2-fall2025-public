{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 329A: Homework 2\n",
    "### Code Generation and Verification\n",
    "\n",
    "In this homework, we will explore the capabilities of LLMs in generating and verifying code. You will implement different test-time verification techniques, from simple zero-shot generation to more advanced methods like multi-candidate sampling and LLM-as-a-judge verification.\n",
    "\n",
    "Objectives:\n",
    "- Establish a baseline for an LLM's code generation performance.\n",
    "- Implement and evaluate the pass@k metric by sampling multiple solutions.\n",
    "- Build an LLM-as-a-Judge to select the best code candidate.\n",
    "- Use an LLM to generate unit tests and perform \"weak verification.\"\n",
    "- Analyze the strengths and weaknesses of each approach.\n",
    "\n",
    "#### Q1: Zero Shot (20 pts)\n",
    "- 1a: Zero Shot Accuracy Code (15 pts)\n",
    "- 1b: Written Analysis (5 pts)\n",
    "\n",
    "#### Q2: Advanced Verification Techniques (40 pts)\n",
    "- 2a: Pass@K Code (10 pts)\n",
    "- 2b: LLM as Judge Code (20 pts)\n",
    "- 2c: Evaluating Judge Code (10 pts)\n",
    "\n",
    "#### Q3: Unit Test Generation and Weak Verifiers (50 pts)\n",
    "- 3a: Unit Test Generator Code (20 pts)\n",
    "- 3a: Evaluate Ground Truth of LLM Tests Code (10 pts)\n",
    "- 3a: Written Analysis (10 pts)\n",
    "- 3b: K-shot Accuracy on LLM tests (10 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs329_hw.tasks import HumanEval\n",
    "from cs329_hw.methods import get_sampler\n",
    "from cs329_hw.run.sandbox_docker import run_python_in_docker\n",
    "from cs329_hw.methods.verifiers import HumanEvalVerifier\n",
    "from cs329_hw.methods.llm_unit_test import LLMUnitTestGeneratorConfig, LLMUnitTestGenerator\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# debug mode runs your code on a subset of the test set for faster iteration\n",
    "DEBUG_MODE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include the following line to reload the modules when you make changes\n",
    "# (helpful when iterating on code locally)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "# input path to .env file, which should contain TOGETHER_API_KEY\n",
    "assert load_dotenv(dotenv_path=\"environment.env\") == True\n",
    "\n",
    "# model path for generation and judging\n",
    "qwen_path = \"together_ai/meta-llama/Llama-3.2-3B-Instruct-Turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HumanEval Dataset Setup\n",
    "We will be working with the [HumanEval](https://arxiv.org/abs/2107.03374) benchmark to evaluate model performance on various coding tasks. The dataset contains 164 problems, each with a function signature, a descriptive docstring, and a set of unit tests to verify the correctness of the generated code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "humaneval = HumanEval()\n",
    "problems = humaneval.get_problems(debug_mode=DEBUG_MODE)\n",
    "system_prompt = humaneval.get_system_prompt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each problem in the dataset is provided as a dictionary containing the problem description, answer, and a set of unit tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_docstring = problems[1][\"problem\"]\n",
    "example_solution = problems[1][\"answer\"]\n",
    "example_test_suite = problems[1][\"test_suite\"]\n",
    "print(\"========Docstring========\", example_docstring)\n",
    "print(\"========Example solution======== \\n\", example_solution)\n",
    "print(\"========Provided tests========\\n\", example_test_suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help you build a few different kinds of verifiers, we have provided the following tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verifier = HumanEvalVerifier(runner=run_python_in_docker, timeout_s=2)\n",
    "\n",
    "# before we run the code, we must combine the function docstring with the solution\n",
    "docstring_and_correct_solution = f\"{example_docstring}\\n{example_solution}\"\n",
    "docstring_and_incorrect_solution = f\"{example_docstring}\\n    return False\"\n",
    "\n",
    "res = verifier.verify( \n",
    "    code=docstring_and_incorrect_solution, # swap this to docstring_and_correct_solution to see what a passing solution output looks like\n",
    "    function_name=problems[1][\"function_name\"],\n",
    "    test_suite=example_test_suite\n",
    ")\n",
    "verifier.print_verification_result(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate code, we'll use a sampler function that can request multiple completions for a single prompt; this is the basis of the techniques that we will implement.\n",
    "\n",
    "The `get_sampler(\"sample_multiple\", n_samples=k, ...)` method returns a list of lists, where for each prompt that is passed as an input, we return a list of `k` responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = get_sampler(\n",
    "    \"sample_multiple\",\n",
    "    qwen_path,\n",
    "    temperature=0.7,\n",
    "    system_prompt=system_prompt,  # IMPORTANT: use the provided system prompt\n",
    "    n_samples=3\n",
    ")\n",
    "prompts = [example_docstring]\n",
    "responses = method(prompts)\n",
    "for i, response in enumerate(responses[0]):\n",
    "    print(f\"========Response {i+1}========\")\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the LLM's output includes Markdown code fences (e.g., ```python``). We need to remove these before we can execute the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_code(text: str) -> str:\n",
    "    \"\"\"Removes python code, demarcated by ```python and ```, from LLM output.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = re.sub(r\"```(?:python)?\\s*\", \"\", text)\n",
    "    text = text.replace(\"```\", \"\")\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "cleaned_responses = [extract_code(resp) for resp in responses[0]]\n",
    "for i, response in enumerate(responses[0]):\n",
    "    print(f\"========Cleaned response {i+1}========\")\n",
    "    print(cleaned_responses[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's verify one of the cleaned responses to see if it's correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = verifier.verify(\n",
    "    code=cleaned_responses[1], # you can edit the index here to try each of the three responses\n",
    "    function_name=problems[1][\"function_name\"],\n",
    "    test_suite=example_test_suite,\n",
    ")\n",
    "verifier.print_verification_result(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Zero-shot predictions (20 pts)\n",
    "- 1a: Zero Shot Accuracy Code (15 pts)\n",
    "- 1b: Written Analysis (5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will evaluate the baseline accuracy of the predictions with a single zero-shot sample.\n",
    "\n",
    "Deliverable:\n",
    "\n",
    "* Write your code in the section specified by `TODO: YOUR CODE STARTS HERE` and `TODO: YOUR CODE ENDS HERE`.\n",
    "* Report the accuracy of the predictions below.\n",
    "\n",
    "Hint: Look at the entries of the verifier.verify() dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(predictions:list[str], problems: list[dict], verifier: HumanEvalVerifier):\n",
    "    \"\"\"\n",
    "    Calculates the zero-shot accuracy of code predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions: A list of generated code strings, one for each problem.\n",
    "        problems: The list of HumanEval problem dictionaries.\n",
    "        verifier: The HumanEvalVerifier instance.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - accuracy (float): The fraction of correctly solved problems (i.e. passed all unit tests).\n",
    "        - response (list[str]): A list of the verifier's `stdout` field, containing the results of each problem.\n",
    "        - wrong (list[int]): A list of indices for the problems that failed.\n",
    "    \"\"\"\n",
    "    ### TODO: YOUR CODE STARTS HERE\n",
    "    \n",
    "    ### TODO: YOUR CODE ENDS HERE\n",
    "    return accuracy, response, incorrect_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = get_sampler(\"sample_multiple\",qwen_path, temperature=0.7, n_samples=1, system_prompt=system_prompt)\n",
    "prompts = [entry[\"problem\"] for entry in problems]\n",
    "predictions_all_probs_zero_shot = method(prompts)\n",
    "cleaned_predictions_zero_shot = [extract_code(raw_code[0]) for raw_code in predictions_all_probs_zero_shot]\n",
    "accuracy, response, wrong = calculate_accuracy(cleaned_predictions_zero_shot, problems, verifier)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Indices of failed problems: {wrong}\")\n",
    "print(f\"Unit test results of all problems: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1b) Experiment with different problems and any failed tests cases. Explain in a few sentences what sorts of patterns do you observe among the failed test cases. What are some possible reasons that the generated code fails these test cases?\n",
    "\n",
    "<span style=\"color:red\">YOUR ANSWER HERE</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Advanced Verification Techniques (40 pts)\n",
    "- 2a: Pass@K Code (10 pts)\n",
    "- 2b: LLM as Judge Code (20 pts)\n",
    "- 2c: Evaluating Judge Code (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In this section, we'll explore more sophisticated techniques to improve our success rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2a) Parallel Sampling and pass@K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of generating just one solution, we want to independently generate `k` different solutions and check if any of them are correct. Here, we use `k=3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = get_sampler(\"sample_multiple\", qwen_path, temperature=0.7, n_samples=3, system_prompt=system_prompt)\n",
    "prompts = [entry[\"problem\"] for entry in problems]\n",
    "preds_3shot = method(prompts)\n",
    "cleaned_preds_3shot = [ # list[list[str]] here, where each inner list[str] contains the k code samples for that problem\n",
    "    [extract_code(code) for code in raw_codes]\n",
    "    for raw_codes in preds_3shot\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the `k_shot_acc` function, which computes pass@k i.e. the proportion of problems for which a correct solution is obtained within `k` attempts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_shot_acc(predictions: list[list[str]], problems: list[dict], verifier):\n",
    "    \"\"\"\n",
    "    Calculates pass@k accuracy. A problem is solved if any of its k candidates pass.\n",
    "\n",
    "    Args:\n",
    "        predictions (list[list[str]]): A list where each item is another list of k code strings for a problem.\n",
    "        problems (list[dict]): The list of HumanEval problem dictionaries.\n",
    "        verifier (HumanEvalVerifier): The HumanEvalVerifier instance.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - accuracy (float): The pass@k accuracy.\n",
    "        - solved_fns (list[str]): A list of function names for problems that were solved.\n",
    "    \"\"\"\n",
    "    num_probs = len(predictions)\n",
    "    num_corr = 0\n",
    "    solved_fns = []\n",
    "\n",
    "    ### TODO: YOUR CODE STARTS HERE\n",
    "\n",
    "    ### TODO: YOUR CODE ENDS HERE\n",
    "    accuracy = num_corr / num_probs if num_probs > 0 else 0.0\n",
    "    return accuracy, solved_fns\n",
    "\n",
    "accuracy_3shot, _ = k_shot_acc(cleaned_preds_3shot, problems, verifier)\n",
    "print(f\"3-shot accuracy: {accuracy_3shot}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe how pass@k varies as we change the number of generated samples for k = 3, 6, and 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "def sample_and_evaluate(k: int):\n",
    "    \"\"\"Samples k code completions per problem and compute accuracy.\"\"\"\n",
    "    method = get_sampler(\n",
    "        \"sample_multiple\",\n",
    "        qwen_path,\n",
    "        temperature=0.7,\n",
    "        n_samples=k,\n",
    "        system_prompt=system_prompt\n",
    "    )\n",
    "    prompts = [entry[\"problem\"] for entry in problems]\n",
    "    preds = method(prompts)\n",
    "    cleaned_preds = [[extract_code(code) for code in raw_codes] for raw_codes in preds]\n",
    "    acc, _ = k_shot_acc(cleaned_preds, problems, verifier)\n",
    "    return acc\n",
    "\n",
    "\n",
    "sample_sizes = [1, 3, 6, 9]\n",
    "accuracies = [accuracy, accuracy_3shot]  # Start with your existing 3-shot accuracy\n",
    "\n",
    "# Run for k = 6 and 9\n",
    "for k in sample_sizes[2:]:\n",
    "    acc = sample_and_evaluate(k)\n",
    "    accuracies.append(acc)\n",
    "    print(f\"Accuracy ({k}-shot): {acc:.3f}\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(sample_sizes, accuracies, marker=\"o\", linewidth=2)\n",
    "plt.title(\"Pass@K\")\n",
    "plt.xlabel(\"K\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2b) LLM-as-a-Judge Verification\n",
    "For some tasks, executing code can be slow and resource-intensive. An alternative is to use another LLM as a \"judge\" to review the candidate solutions and select the one it deems most likely to be correct. This leverages the model's understanding of code quality and logic without requiring execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to implement the `judge` and `_build_messages` methods in the `LLMJudge` class.\n",
    "\n",
    "- `_build_messages`: Construct the prompt that will be sent to the judge LLM. It should include the problem specification and the formatted candidate solutions.\n",
    "\n",
    "- `judge`: Use the sampler to send the prompt to the judge and parse its JSON response to extract the chosen index and reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable, List, Dict, Any, Optional, Tuple\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from cs329_hw.methods.simple_samplers import SampleMultiple\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class JudgeConfig:\n",
    "    temperature: float = 0.7   # sampling temp you set inside your sampler\n",
    "    max_choices: int = 10      # safety cap for number of candidate codes\n",
    "\n",
    "class LLMJudge:\n",
    "    \"\"\"\n",
    "    LLM-as-a-judge that selects the index of the best code snippet from a list of candidate completions.\n",
    "    If no candidate seems correct, the LLM returns None.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sampler: SampleMultiple, cfg: JudgeConfig = JudgeConfig(), model_name: str = qwen_path):\n",
    "        self.sampler = sampler\n",
    "        self.cfg = cfg\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def judge(\n",
    "        self,\n",
    "        problem_prompt: str,\n",
    "        function_name: str,\n",
    "        code_snippets: List[str],\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Orchestrates the judging process by building a prompt, querying the LLM, and parsing the response.\n",
    "\n",
    "        Your implementation should follow these steps:\n",
    "        1. Call `self._build_messages()` to construct the detailed prompt for the LLM-based judge, which \n",
    "            includes the code snippets to be judged.\n",
    "        2. Use `self.sampler` to send this prompt to the LLM and get its raw response.\n",
    "        3. Call `self._parse_json_choice()` to robustly extract the judge's decision from the raw text.\n",
    "            \n",
    "        Args:\n",
    "            problem_prompt (str): The problem specification.\n",
    "            function_name (str): The name of the target function.\n",
    "            code_snippets (List[str]): A list of candidate code snippets.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing:\n",
    "            - \"choice\": the int index of the chosen code snippet, or None\n",
    "            - \"reason\": the stripped string that describes why the model chose the option\n",
    "            - \"raw_response\": the raw response from the LLM for debugging\n",
    "        \"\"\"\n",
    "        assert 1 <= len(code_snippets) <= self.cfg.max_choices\n",
    "\n",
    "        ### TODO: YOUR CODE STARTS HERE\n",
    "\n",
    "        ### TODO: YOUR CODE ENDS HERE\n",
    "        \n",
    "        return {\"choice\": final_choice, \"reason\": reason, \"raw_response\": raw_response}\n",
    "\n",
    "    def _build_messages(self, problem_prompt: str, function_name: str, code_snippets: List[str]) -> str:\n",
    "        \"\"\"\n",
    "        Builds the full text prompt for the LLM-as-a-judge model.\n",
    "\n",
    "        Your prompt should include:\n",
    "        - A message describing the LLM judge's behavior as a code evaluator\n",
    "        - The problem statement (`problem_prompt`) and target function name (`function_name`)\n",
    "        - The list of all candidate code snippets the LMM judge will choose from (`code_snippets`)\n",
    "        - Instructions about choosing the most correct code\n",
    "        - Examples of the expected response format.\n",
    "          \n",
    "        The LLM judge should be prompted to return a single-line JSON object with this exact schema (no prose before/after):\n",
    "        {{\n",
    "        \"choice\": <integer index or null>,\n",
    "        \"reason\": \"<one short sentence>\"\n",
    "        }}\n",
    "\n",
    "        Expected Output:\n",
    "          The returned string should contain both the system prompt and user instructions,\n",
    "          ready to be passed into the LLM sampler.\n",
    "        \"\"\"\n",
    "\n",
    "        ### TODO: YOUR CODE STARTS HERE\n",
    "\n",
    "        ### TODO: YOUR CODE ENDS HERE\n",
    "        \n",
    "        return prompt\n",
    "        \n",
    "\n",
    "\n",
    "    def _parse_json_choice(self, raw: str) -> Tuple[Optional[int], str]:\n",
    "        \"\"\"\n",
    "        Robustly extracts the judge's choice and reason from the raw LLM text response.\n",
    "\n",
    "        Args:\n",
    "            raw (str): The raw text output from the LLM judge.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing:\n",
    "                - Optional[int]: The chosen index (or None if unparseable, None-chosen, or invalid)\n",
    "                - str: The LLM's reasoning for the choice\n",
    "        \"\"\"\n",
    "        if not raw or not raw.strip():\n",
    "            return None, \"Empty response\"\n",
    "\n",
    "        first = raw.strip().splitlines()[0].strip()\n",
    "        obj = None\n",
    "        try:\n",
    "            obj = json.loads(first)\n",
    "        except Exception:\n",
    "            m = re.search(r\"\\{.*\\}\", raw, flags=re.DOTALL)\n",
    "            if m:\n",
    "                try:\n",
    "                    obj = json.loads(m.group(0))\n",
    "                except Exception:\n",
    "                    obj = None\n",
    "\n",
    "        if not isinstance(obj, dict):\n",
    "            return None, \"Unparseable\"\n",
    "\n",
    "        choice = obj.get(\"choice\", None)\n",
    "        reason = obj.get(\"reason\", \"\")\n",
    "        if choice is None:\n",
    "            return None, reason or \"None\"\n",
    "\n",
    "        try:\n",
    "            idx = int(choice)\n",
    "            return (idx if idx >= 0 else None), reason\n",
    "        except Exception:\n",
    "            return None, reason or \"Non-integer index\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test this implementation, we can apply the LLM-as-a-judge to the 3 responses we sampled for the first HumanEval problem in the LLM Sampling section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_method = get_sampler(\"sample_multiple\", qwen_path, temperature=1, n_samples=1)\n",
    "\n",
    "judge = LLMJudge(sampler=judge_method, cfg=JudgeConfig(temperature=0.7), model_name=qwen_path)\n",
    "\n",
    "candidates = cleaned_preds_3shot[0]\n",
    "decision = judge.judge(problems[0][\"problem\"], problems[0][\"function_name\"], candidates)\n",
    "print(\"Problem:\", problems[0][\"problem\"])\n",
    "for i, candidate in enumerate(candidates):\n",
    "    print(f\"Candidate {i}: {candidate}\")\n",
    "    print(\"---\"*50)\n",
    "print(\"Judge choice:\", decision[\"choice\"])\n",
    "print(\"Judge reason:\", decision[\"reason\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluating the Judge**\n",
    "\n",
    "Now, let's write a function to loop through all our problems, use the judge to select the best of the three candidates we generated earlier, and then calculate the final accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_judge(problems: list[dict], code_generations: list[list[str]], judge:LLMJudge) -> list[str]:\n",
    "    \"\"\"\n",
    "    Evaluates a set of generated code solutions using a given LLM judge.\n",
    "\n",
    "    Args:\n",
    "        problems (list[dict]): The list of HumanEval problem dictionaries.\n",
    "        code_generations (list[list[str]]): A list where each element is a list of code samples for the corresponding problem.\n",
    "        judge (LLMJudge): LLMJudge object\n",
    "\n",
    "    Returns:\n",
    "        A list containing the code snippet chosen by the judge for each problem.\n",
    "        If the judge did not make a choice for a given problem, the corresponding\n",
    "        element in the list will be `None`.\n",
    "    \"\"\"\n",
    "    ### TODO: YOUR CODE STARTS HERE\n",
    "    \n",
    "    ### TODO: YOUR CODE ENDS HERE\n",
    "    return results\n",
    "\n",
    "code_generation_method = get_sampler(\n",
    "    \"sample_multiple\",\n",
    "    qwen_path,\n",
    "    temperature=0.7,\n",
    "    n_samples=3,\n",
    "    system_prompt=system_prompt\n",
    ")\n",
    "judge_method = get_sampler(\"sample_multiple\", qwen_path, temperature=1, n_samples=1)\n",
    "judge = LLMJudge(sampler=judge_method, cfg=JudgeConfig(temperature=0.7), model_name=qwen_path)\n",
    "\n",
    "cleaned_predictions_llm_judge = evaluate_judge(problems, cleaned_preds_3shot, judge)\n",
    "accuracy, response, wrong = calculate_accuracy(cleaned_predictions_llm_judge, problems, verifier)\n",
    "print(f\"LLM Judge accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Unit Test Generation and Weak Verifiers (50 pts)\n",
    "- 3a: Unit Test Generator Code (20 pts)\n",
    "- 3a: Evaluate Ground Truth of LLM Tests Code (10 pts)\n",
    "- 3a: Written Analysis (10 pts)\n",
    "- 3b: K-shot Accuracy on LLM tests (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we've relied on the ground-truth test suite. In many real-world cases, we don't have access to this ground-truth test suite. In this section, we'll use an LLM to generate its own unit tests. This enables a pipeline with \"weak verifiers\", where we use these synthetic tests to filter and select code solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3a: Generating and Validating Unit Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's assess how good the LLM is at writing tests. We will prompt it to generate 5 test cases for each problem based only on the docstring. Then, we'll run these tests against the ground-truth solution. The percentage of problems where the ground-truth code passes all synthetic tests gives us a measure of the LLM's ability to generate reliable tests.\n",
    "\n",
    "**Note**: We use a more powerful model for test generation, as it is a more demanding task.\n",
    "\n",
    "**Deliverable:** Fill out `LLMUnitTestGenerator._build_prompt()` and `LLMUnitTestGenerator.generate()` in `methods/llm_unit_test.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_large = \"together_ai/Qwen/Qwen2.5-72B-Instruct-Turbo\"\n",
    "\n",
    "# Here is an example of using the LLM-based unit test generator to create tests for one problem.\n",
    "testgen_method = get_sampler(\n",
    "    \"sample_multiple\",\n",
    "    qwen_large, \n",
    "    temperature=1.0,\n",
    "    n_samples=1,\n",
    "    system_prompt=\"You are a careful Python unit test designer.\"\n",
    ")\n",
    "testgen = LLMUnitTestGenerator(sampler=testgen_method, cfg=LLMUnitTestGeneratorConfig())\n",
    "\n",
    "doc = problems[0][\"problem\"]\n",
    "fn  = problems[0][\"function_name\"]\n",
    "\n",
    "cases = testgen.generate(problem_prompt=doc, function_name=fn, n_unit_tests=5)\n",
    "candidate_code = f\"{doc}\\n{problems[0]['answer']}\"\n",
    "\n",
    "res = verifier.verify(code=candidate_code, function_name=fn, test_suite=cases)\n",
    "HumanEvalVerifier.print_verification_result(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ground_truth_on_llm_unit_tests(\n",
    "    problems: list[dict],\n",
    "    llm_unit_test_generator: LLMUnitTestGenerator,\n",
    "    verifier: HumanEvalVerifier,\n",
    "    n_unit_tests: int = 5\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates unit tests using the LLMUnitTestGenerator and evaluates the ground-truth code against them.\n",
    "\n",
    "    Returns:\n",
    "        - accuracy (float): Percentage of problems where ground truth passed all synthetic tests.\n",
    "        - correct_idxs (list[int]): The indices of the problems that passed the tests.\n",
    "        - generated_unit_tests (list[list[TestCase]]): A list of test cases for each problem, where the outer list is over all problems.\n",
    "    \"\"\"\n",
    "    num_problems = len(problems)\n",
    "    num_correct = 0\n",
    "    correct_idxs = []\n",
    "    generated_unit_tests = []\n",
    "\n",
    "    ### TODO: YOUR CODE STARTS HERE\n",
    "    \n",
    "    ### TODO: YOUR CODE ENDS HERE\n",
    "\n",
    "    accuracy = num_correct / num_problems if num_problems > 0 else 0.0\n",
    "    print(f\"Ground truth code passed all LLM-generated tests for {num_correct}/{num_problems} problems.\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    return accuracy, correct_idxs, generated_unit_tests\n",
    "\n",
    "accuracy, correct_idxs, generated_unit_tests = evaluate_ground_truth_on_llm_unit_tests(problems, testgen, verifier, n_unit_tests=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the LLM-generated unit tests are sometimes unreliable, where the ground-truth code fails to pass the synthetic tests. Let's examine one of these failures to see what went wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "incorrect_idxs = set(range(len(problems))) - set(correct_idxs)\n",
    "incorrect_idx = random.choice(list(incorrect_idxs))\n",
    "incorrect_problem_data = problems[incorrect_idx]\n",
    "\n",
    "print(f\"Problem: {incorrect_problem_data['problem']}\", \"-\"*100)\n",
    "doc, ans = incorrect_problem_data['problem'], incorrect_problem_data['answer']\n",
    "gt_code = f\"{doc}\\n{ans}\"\n",
    "for i, test in enumerate(generated_unit_tests[incorrect_idx]):\n",
    "    print(f\"LLM-generated unit test {i}: {test}\")\n",
    "print()\n",
    "res =verifier.verify(code=gt_code, function_name=incorrect_problem_data['function_name'], test_suite=generated_unit_tests[incorrect_idx])\n",
    "HumanEvalVerifier.print_verification_result(res)\n",
    "print(\"-\"*100)\n",
    "print(\"Ground truth code:\")\n",
    "print(gt_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analyze at least one of the mismatches determine which category(s) it falls into. Explain your reasoning with specific details and example from the test cases.\n",
    "- **Misinterpreting nuanced requirements:** LLM grasps main goal but fails to apply subtle details like secondary conditions or tie-breaking rules.\n",
    "- **Flawed algorithmic simulation:** Model cannot reliably execute a multi-step algorithm internally. Instead of computing the true result (e.g. a full Collatz sequence), it produces a (wrong) statistically likely output.\n",
    "- **Overgeneralization from Training Data:** LLM applies solution pattern from similar but distinct example test cases in docstring. The generated test is valid for that other problem, but not for the specific function provided.\n",
    "\n",
    "<span style=\"color:red\">YOUR ANSWER HERE</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b) LLM-generated unit tests as weak verifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've identified a subset of problems for which our LLM generated reliable tests (`passed_problems`), we can use them to select the best code candidate. This simulates a realistic scenario where we don't have a human-written test suite and must rely on our synthetic tests to select the best code candidate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this, we'll conduct a direct comparison on this trusted subset. For each of these problems, we'll use the three candidate solutions earlier in the pass@3 experiment. We'll then evaluate their correctness using two different methods:\n",
    "\n",
    "1. **Baseline with Oracle Ground-Truth Tests:** First, we'll calculate the pass@k accuracy on this subset using the original, human-written test suites. This gives us the true, \"best possible\" score for our candidate solutions and serves as our gold standard for comparison.\n",
    "\n",
    "2. **Evaluation with Synthetic LLM-Generated Tests (Weak Verifier)**: Next, we will perform the same calculation using our trusted, LLM-generated test suites (`passed_testcases`) for verification. The result will tell us how effectively our automated pipeline can identify correct code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_shot_acc_synthetic(\n",
    "    predictions: list[list[str]],\n",
    "    problems: list[dict],\n",
    "    unit_tests: list[list[TestCase]],\n",
    "    verifier: Verifier\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluates model accuracy using LLM-generated (synthetic) test cases.\n",
    "    A problem is considered solved if any of its candidate solutions pass all synthetic tests.\n",
    "\n",
    "    Args:\n",
    "        predictions (list[list[str]]): A list of code predictions for each problem, where each inner list contains k code samples\n",
    "        problems (list[dict]): A list of problem dicts\n",
    "        unit_tests (list[list[TestCase]]): A list of LLM-generated test cases for each problem\n",
    "        verifier (Verifier): Verifier object\n",
    "\n",
    "    Returns:\n",
    "    A tuple containing:\n",
    "        - The pass@k accuracy, i.e. the fraction of problems for which at least one candidate solution passed all the LLM-generated unit tests\n",
    "        - A list of function names for the problems that were successfully solved.\n",
    "    \"\"\"\n",
    "    num_probs = len(predictions)\n",
    "    num_corr = 0\n",
    "    correct_fns = []\n",
    "\n",
    "    assert len(predictions) == len(problems) == len(unit_tests)\n",
    "    ### TODO: YOUR CODE STARTS HERE\n",
    "\n",
    "    ### TODO: YOUR CODE ENDS HERE\n",
    "    accuracy = num_corr / num_probs if num_probs > 0 else 0.0\n",
    "    return accuracy, correct_fns\n",
    "\n",
    "subset_preds_3_shot = [cleaned_preds_3shot[i] for i in correct_idxs]\n",
    "subset_problems = [problems[i] for i in correct_idxs]\n",
    "subset_testcases = [generated_unit_tests[i] for i in correct_idxs]\n",
    "accuracy_true, passed_true = k_shot_acc(subset_preds_3_shot, subset_problems, verifier)\n",
    "accuracy_synth, passed_synth = k_shot_acc_synthetic(subset_preds_3_shot, subset_problems, subset_testcases, verifier)\n",
    "\n",
    "print(f\"Evaluating pass@3 accuracy on subset of {len(correct_idxs)} problems (where ground truth code passed all LLM-generated tests)\")\n",
    "print(f\"pass@3 with ground truth tests: {accuracy_true:.2f}\")\n",
    "print(f\"pass@3 with LLM-generated tests: {accuracy_synth:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refining Unit Tests\n",
    "\n",
    "To increase the quality of generated tests, we can provide the LLM with more context (e.g. the ground truth code, a summary of the program's logic, or a description of intended behavior and potential edge cases). This paper provides a good overview of this topic: https://arxiv.org/pdf/2502.01619"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs329-hw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
